{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1Ci_qWVKr8XT4bWU8puT1vgodW9V7Tl8H",
      "authorship_tag": "ABX9TyOfQR3Sh4dC8dP+dHDvqe6q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harshavardhansannithi/ARDS-using-Machine-learning/blob/main/DT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gBNpddJC14X7",
        "outputId": "d20365c3-fa82-4471-e327-32c40f1be215"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN:\n",
            "Precision: 0.25\n",
            "Recall: 0.041666666666666664\n",
            "F1-score: 0.07142857142857142\n",
            "The accuracy score achieved using KNN is: 81.56 %\n",
            "\n",
            "Decision Tree:\n",
            "Precision: 0.0\n",
            "Recall: 0.0\n",
            "F1-score: 0.0\n",
            "The accuracy score achieved using KNN is: 79.43 %\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Load your dataset (replace with your actual data)\n",
        "data = pd.read_csv(\"/content/drive/MyDrive/lung_disease.csv\")\n",
        "\n",
        "# Separate features and target variable\n",
        "X = data.drop(\"Risk\", axis=1)\n",
        "y = data[\"Risk\"]\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming X is your feature matrix\n",
        "# Identify and encode categorical columns\n",
        "categorical_columns = X.select_dtypes(include=['object']).columns\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "for col in categorical_columns:\n",
        "    X[col] = label_encoder.fit_transform(X[col])\n",
        "\n",
        "# Now, you can proceed with feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# X_scaled is now your scaled feature matrix\n",
        "\n",
        "\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# KNN Model\n",
        "knn_clf = KNeighborsClassifier(n_neighbors=5)  # Experiment with different k values\n",
        "knn_clf.fit(X_train, y_train)\n",
        "y_pred_knn = knn_clf.predict(X_test)\n",
        "\n",
        "# Decision Tree Model\n",
        "dt_clf = DecisionTreeClassifier(max_depth=5)  # Experiment with hyperparameters\n",
        "dt_clf.fit(X_train, y_train)\n",
        "y_pred_dt = dt_clf.predict(X_test)\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Assuming y_test and y_pred_knn are your true labels and predicted labels for KNN\n",
        "print(\"KNN:\")\n",
        "print(f\"Precision: {precision_score(y_test, y_pred_knn, pos_label='T')}\")\n",
        "print(f\"Recall: {recall_score(y_test, y_pred_knn, pos_label='T')}\")\n",
        "print(f\"F1-score: {f1_score(y_test, y_pred_knn, pos_label='T')}\")\n",
        "\n",
        "#Accuracy\n",
        "score_knn = round(accuracy_score(y_test, y_pred_knn)*100,2)\n",
        "print(\"The accuracy score achieved using KNN is: \"+str(score_knn)+\" %\")\n",
        "\n",
        "\n",
        "print(\"\\nDecision Tree:\")\n",
        "print(f\"Precision: {precision_score(y_test, y_pred_dt,pos_label='T')}\")\n",
        "print(f\"Recall: {recall_score(y_test, y_pred_dt,pos_label='T')}\")\n",
        "print(f\"F1-score: {f1_score(y_test, y_pred_dt,pos_label='T')}\")\n",
        "\n",
        "#Accuracy\n",
        "score_dt = round(accuracy_score(y_test, y_pred_dt)*100,2)\n",
        "print(\"The accuracy score achieved using KNN is: \"+str(score_dt)+\" %\")\n",
        "\n",
        "# Hyperparameter tuning and other comparisons (optional)\n",
        "# - Experiment with different k values for KNN and hyperparameters for Decision Tree (max_depth, min_samples_split, etc.).\n",
        "# - Use cross-validation for more robust evaluation.\n",
        "# - Consider additional metrics like AUC-ROC, confusion matrix, or interpretability measures.\n"
      ]
    }
  ]
}